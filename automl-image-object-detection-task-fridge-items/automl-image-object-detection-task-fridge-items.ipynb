{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AutoML: 「最適な」画像オブジェクト検出モデルを学習する\n",
        "\n",
        "**目的** - AutoML の画像オブジェクト検出ジョブの設定と実行方法について説明します。これは AutoML がサポートする9つの ML タスクのうちの1つです。他の ML タスクには、「予測」、「分類」、「画像オブジェクト検出」、「NLP テキスト分類」などがあります。\n",
        "\n",
        "このノートブックでは、小さなデータセットを使って AutoML によりモデルを学習し、モデルの性能を最適化するためにモデルのハイパーパラメータを調整する方法を示し、推論シナリオで使用するモデルをデプロイします。\n",
        "\n",
        "**前提条件** - このハンズオンでは、以下が前提条件となります:\n",
        "- Machine Learning の基礎知識\n",
        "- 利用可能な Azure subscription. [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F)\n",
        "- Azure ML workspace [Check this notebook for creating a workspace](../../../resources/workspace/workspace.ipynb) \n",
        "- Compute Cluster [Check this notebook to create a compute cluster](../../../resources/compute/compute.ipynb)\n",
        "- Python 実行環境\n",
        "- Azure Machine Learning Python SDK v2 - [install instructions](../../../README.md)\n",
        "\n",
        "**学習のゴール** - このハンズオンを完了すると、以下を学習できます:\n",
        "- Python SDK から AML workspacce への接続方法\n",
        "- 'image_object_detection()' 関数を使って `AutoML Image Object Detection Training Job` を実行する方法\n",
        "- Azure Machine Learning のコンピュートで AutoML 機能を使ってモデルを構築する方法\n",
        "- 学習済みモデルと推論スコアの取得 "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Azure Machine Learning Workspace への接続\n",
        "\n",
        "[Workspace](https://docs.microsoft.com/en-us/azure/machine-learning/concept-workspace) は Azure Machine Learning における最上位のリソースで、チームにおける機械学習成果物を作成し、関連作業をグループ化して管理する場所です。 実験、ジョブ、データセット、モデル、コンポーネント、推論エンドポイントなどの機械学習開発における成果物を統合し、共同作業を効率化することが可能です。\n",
        "このセクションでは、ジョブを実行する Workspace に接続する。\n",
        "\n",
        "## 1.1. Python ライブラリの import"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# 必要なライブラリのインポート\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.ml import MLClient\n",
        "\n",
        "from azure.ai.ml.automl import SearchSpace, ObjectDetectionPrimaryMetrics\n",
        "from azure.ai.ml.sweep import (\n",
        "    Choice,\n",
        "    Uniform,\n",
        "    BanditPolicy,\n",
        ")\n",
        "\n",
        "from azure.ai.ml import automl"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1712796315815
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2. Workspace の詳細を設定し接続する\n",
        "\n",
        "ワークスペースに接続するには、パラメータ情報（サブスクリプション、リソースグループ、ワークスペース名）が必要です。これらの情報を `azure.ai.ml` の `MLClient` で使用して、必要な Azure Machine Learning ワークスペースへ接続して各種操作を可能にします。このチュートリアルでは、デフォルトの [default azure authentication](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python) を使用します。\n",
        "\n",
        "パラメータ情報（サブスクリプション、リソースグループ、ワークスペース名）を直接指定して接続することも可能です。パラメータについては、ワークスペースの右上のタブから確認することが可能です。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "credential = DefaultAzureCredential()\n",
        "ml_client = None\n",
        "try:\n",
        "    ml_client = MLClient.from_config(credential)\n",
        "except Exception as ex:\n",
        "    print(ex)\n",
        "    # Enter details of your AML workspace\n",
        "    subscription_id = \"<SUBSCRIPTION_ID>\"\n",
        "    resource_group = \"<RESOURCE_GROUP>\"\n",
        "    workspace = \"<AML_WORKSPACE_NAME>\"\n",
        "    ml_client = MLClient(credential, subscription_id, resource_group, workspace)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1712796316316
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "validation-workspace"
        ]
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. MLTable と入力トレーニングデータ\n",
        "\n",
        "AutoML を用いてコンピュータビジョンのモデルを生成するには、モデルトレーニングの入力としてラベル付き画像データを MLTable の形式で提供する必要があります。MLTable は、様々なデータソースからのデータを統一的に扱うための抽象化された論理的なデータフォーマットを定義することが可能です。\n",
        "\n",
        "\n",
        "ラベル付きトレーニングデータは JSONL 形式で MLTable を作成することができます。もしラベル付きトレーニングデータが異なる形式（例えば、pascal VOCやCOCOなど）である場合、まずそれをJSONL形式に変換するための変換スクリプトを使用し、その後で MLTable を作成できます。または、Azure Machine Learning のデータラベリングツールを使用して手動で画像にラベルを付け、ラベル付けされたデータをエクスポートして AutoML モデルのトレーニングに使用することもできます。\n",
        "\n",
        "このノートブックでは、「Fridge Objects」とのデータセットを使用しています。これは、異なるシーンで撮影された4つのラベル（缶、紙パック、ミルクボトル、水ボトル）の飲料容器の写真128枚から構成されています。\n",
        "このノートブックのすべての画像は[このリポジトリ](https://github.com/microsoft/computervision-recipes)にホストされており、[MIT license](https://github.com/microsoft/computervision-recipes/blob/master/LICENSE)の下で利用可能です。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. データのダウンロード\n",
        "まず、データをローカルにダウンロードして解凍します。デフォルトでは、データは現在のディレクトリ内の./dataフォルダにダウンロードされます。\n",
        "\n",
        "データを別の場所にダウンロードしたい場合は、次のセルのdataset_parent_dir = ...を更新してください。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Change to a different location if you prefer\n",
        "dataset_parent_dir = \"./data\"\n",
        "\n",
        "# create data folder if it doesnt exist.\n",
        "os.makedirs(dataset_parent_dir, exist_ok=True)\n",
        "\n",
        "# download data\n",
        "download_url = \"https://cvbp-secondary.z19.web.core.windows.net/datasets/object_detection/odFridgeObjects.zip\"\n",
        "\n",
        "# Extract current dataset name from dataset url\n",
        "dataset_name = os.path.split(download_url)[-1].split(\".\")[0]\n",
        "# Get dataset path for later use\n",
        "dataset_dir = os.path.join(dataset_parent_dir, dataset_name)\n",
        "\n",
        "# Get the data zip file path\n",
        "data_file = os.path.join(dataset_parent_dir, f\"{dataset_name}.zip\")\n",
        "\n",
        "# Download the dataset\n",
        "urllib.request.urlretrieve(download_url, filename=data_file)\n",
        "\n",
        "# extract files\n",
        "with ZipFile(data_file, \"r\") as zip:\n",
        "    print(\"extracting files...\")\n",
        "    zip.extractall(path=dataset_parent_dir)\n",
        "    print(\"done\")\n",
        "# delete zip file\n",
        "os.remove(data_file)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1712796331997
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "データセット内のサンプル画像データを確認"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "sample_image = os.path.join(dataset_dir, \"images\", \"31.jpg\")\n",
        "Image(filename=sample_image)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1712796332664
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. AML データアセット（URIフォルダ）としてデータストアに画像をアップロードする。\n",
        "\n",
        "Azure ML でモデルトレーニングにデータを利用するために、ワークスペースのデフォルト Blob Storage にデータをアップロードします。\n",
        "\n",
        "[Check this notebook for AML data asset example](../../../assets/data/data.ipynb)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Uploading image files by creating a 'data asset URI FOLDER':\n",
        "\n",
        "from azure.ai.ml.entities import Data\n",
        "from azure.ai.ml.constants import AssetTypes, InputOutputModes\n",
        "from azure.ai.ml import Input\n",
        "\n",
        "my_data = Data(\n",
        "    path=dataset_dir,\n",
        "    type=AssetTypes.URI_FOLDER,\n",
        "    description=\"Fridge-items images Object detection\",\n",
        "    name=\"fridge-items-images-object-detection\",\n",
        ")\n",
        "\n",
        "uri_folder_data_asset = ml_client.data.create_or_update(my_data)\n",
        "\n",
        "print(uri_folder_data_asset)\n",
        "print(\"\")\n",
        "print(\"Path to folder in Blob Storage:\")\n",
        "print(uri_folder_data_asset.path)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "upload-data",
        "gather": {
          "logged": 1712796339028
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3. ダウンロードしたデータを JSONL 形式に変換\n",
        "\n",
        "この例では、冷蔵庫オブジェクトのデータセットが Pascal VOC 形式でアノテーションされており、各画像にはxmlファイルが対応しています。各xmlファイルには、対応する画像ファイルの場所と、バウンディングボックスおよびオブジェクトラベルに関する情報が含まれています。\n",
        "\n",
        "このデータを AzureML MLTable で使用するためには、特定の JSONL 形式に変換する必要があります。以下のスクリプトは、対応する MLTable フォルダ内に2つの .jsonl ファイル（1つはトレーニング用、もう1つはバリデーション用）を作成します。トレーニング/バリデーションの比率は、4:1 とします。AutoML で画像分類タスクに使用される jsonl ファイルの詳細については、[ドキュメント](https://learn.microsoft.com/en-us/azure/machine-learning/reference-automl-images-schema#object-detection)を参照してください。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## JSONLファイルを生成する"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "JSONL 変換コードは pycocotools と simplification パッケージを必要とします。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!/anaconda/envs/azureml_py310_sdkv2/bin/pip install pycocotools\n",
        "!/anaconda/envs/azureml_py310_sdkv2/bin/pip install simplification\n",
        "!/anaconda/envs/azureml_py310_sdkv2/bin/pip install scikit-image"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!/anaconda/envs/azureml_py310_sdkv2/bin/pip install torch==1.12.0+cu102 torchvision==0.13.0+cu102 torchaudio==0.12.0 --extra-index-url https://download.pytorch.org/whl/cu102"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "sys.path.insert(0, \"../jsonl-conversion/\")\n",
        "from base_jsonl_converter import write_json_lines\n",
        "from voc_jsonl_converter import VOCJSONLConverter\n",
        "\n",
        "base_url = os.path.join(uri_folder_data_asset.path, \"images/\")\n",
        "converter = VOCJSONLConverter(base_url, os.path.join(dataset_dir, \"annotations\"))\n",
        "jsonl_annotations = os.path.join(dataset_dir, \"annotations_voc.jsonl\")\n",
        "write_json_lines(converter, jsonl_annotations)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1712796364125
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## トレーニング/バリデーションにデータを分割"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# We'll copy each JSONL file within its related MLTable folder\n",
        "training_mltable_path = os.path.join(dataset_parent_dir, \"training-mltable-folder\")\n",
        "validation_mltable_path = os.path.join(dataset_parent_dir, \"validation-mltable-folder\")\n",
        "\n",
        "# First, let's create the folders if they don't exist\n",
        "os.makedirs(training_mltable_path, exist_ok=True)\n",
        "os.makedirs(validation_mltable_path, exist_ok=True)\n",
        "\n",
        "train_validation_ratio = 5\n",
        "\n",
        "# Path to the training and validation files\n",
        "train_annotations_file = os.path.join(training_mltable_path, \"train_annotations.jsonl\")\n",
        "validation_annotations_file = os.path.join(\n",
        "    validation_mltable_path, \"validation_annotations.jsonl\"\n",
        ")\n",
        "\n",
        "with open(jsonl_annotations, \"r\") as annot_f:\n",
        "    json_lines = annot_f.readlines()\n",
        "\n",
        "index = 0\n",
        "with open(train_annotations_file, \"w\") as train_f:\n",
        "    with open(validation_annotations_file, \"w\") as validation_f:\n",
        "        for json_line in json_lines:\n",
        "            if index % train_validation_ratio == 0:\n",
        "                # validation annotation\n",
        "                validation_f.write(json_line)\n",
        "            else:\n",
        "                # train annotation\n",
        "                train_f.write(json_line)\n",
        "            index += 1"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1712796364495
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4. アノテーションファイルを COCO からJSONL に変換する\n",
        "COCO 形式のデータセットを使ってみたい場合は、以下のスクリプトで`jsonl`形式に変換することができます。odFridgeObjects_coco.json ファイルは `odFridgeObjects` データセットのアノテーション情報から構成されています。"
      ],
      "metadata": {
        "tags": [
          "validation-remove"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "sys.path.insert(0, \"../jsonl-conversion/\")\n",
        "from base_jsonl_converter import write_json_lines\n",
        "from coco_jsonl_converter import COCOJSONLConverter\n",
        "\n",
        "base_url = os.path.join(uri_folder_data_asset.path, \"images/\")\n",
        "print(base_url)\n",
        "converter = COCOJSONLConverter(base_url, \"./odFridgeObjects_coco.json\")\n",
        "jsonl_annotations = os.path.join(dataset_dir, \"annotations_coco.jsonl\")\n",
        "write_json_lines(converter, jsonl_annotations)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [
          "validation-remove"
        ],
        "gather": {
          "logged": 1712796364845
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### バウンディング・ボックスの可視化\n",
        "以下の[チュートリアル](https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-auto-train-image-models#visualize-data)の \"データの可視化 \"セクションを参照して、トレーニングを開始する前に、グランドトゥルースのバウンディングボックスを簡単に可視化する方法を確認してください。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5. MLTable 作成\n",
        "上記で作成したjsonlファイルを使ってMLTableデータ入力を作成します。\n",
        "\n",
        "このノートブック以外のジョブのために独自の MLTable アセットを作成するドキュメントについては、以下のリソースを参照してください。\n",
        "- [MLTable YAML Schema](https://learn.microsoft.com/en-us/azure/machine-learning/reference-yaml-mltable) - covers how to write MLTable YAML, which is required for each MLTable asset.\n",
        "- [Create MLTable data asset](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-data-assets?tabs=Python-SDK#create-a-mltable-data-asset) - covers how to create MLTable data asset. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def create_ml_table_file(filename):\n",
        "    \"\"\"Create ML Table definition\"\"\"\n",
        "\n",
        "    return (\n",
        "        \"paths:\\n\"\n",
        "        \"  - file: ./{0}\\n\"\n",
        "        \"transformations:\\n\"\n",
        "        \"  - read_json_lines:\\n\"\n",
        "        \"        encoding: utf8\\n\"\n",
        "        \"        invalid_lines: error\\n\"\n",
        "        \"        include_path_column: false\\n\"\n",
        "        \"  - convert_column_types:\\n\"\n",
        "        \"      - columns: image_url\\n\"\n",
        "        \"        column_type: stream_info\"\n",
        "    ).format(filename)\n",
        "\n",
        "\n",
        "def save_ml_table_file(output_path, mltable_file_contents):\n",
        "    with open(os.path.join(output_path, \"MLTable\"), \"w\") as f:\n",
        "        f.write(mltable_file_contents)\n",
        "\n",
        "\n",
        "# Create and save train mltable\n",
        "train_mltable_file_contents = create_ml_table_file(\n",
        "    os.path.basename(train_annotations_file)\n",
        ")\n",
        "save_ml_table_file(training_mltable_path, train_mltable_file_contents)\n",
        "\n",
        "# Save train and validation mltable\n",
        "validation_mltable_file_contents = create_ml_table_file(\n",
        "    os.path.basename(validation_annotations_file)\n",
        ")\n",
        "save_ml_table_file(validation_mltable_path, validation_mltable_file_contents)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1712796365218
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import Input\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "# Training MLTable defined locally, with local data to be uploaded\n",
        "my_training_data_input = Input(type=AssetTypes.MLTABLE, path=training_mltable_path)\n",
        "\n",
        "# Validation MLTable defined locally, with local data to be uploaded\n",
        "my_validation_data_input = Input(type=AssetTypes.MLTABLE, path=validation_mltable_path)\n",
        "\n",
        "# WITH REMOTE PATH: If available already in the cloud/workspace-blob-store\n",
        "# my_training_data_input = Input(type=AssetTypes.MLTABLE, path=\"azureml://datastores/workspaceblobstore/paths/vision-classification/train\")\n",
        "# my_validation_data_input = Input(type=AssetTypes.MLTABLE, path=\"azureml://datastores/workspaceblobstore/paths/vision-classification/valid\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "data-load",
        "gather": {
          "logged": 1712796365619
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "V1 SDK を使用して作成した TabularDataset からデータ入力を作成するには、`type`に `AssetTypes.MLTABLE`、`mode`に `InputOutputModes.DIRECT`、`path`に `azureml:<tabulardataset_name>:<version>` という形式で指定します。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Training MLTable with v1 TabularDataset\n",
        "my_training_data_input = Input(\n",
        "    type=AssetTypes.MLTABLE, path=\"azureml:odFridgeObjectsTrainingDataset:1\",\n",
        "    mode=InputOutputModes.DIRECT\n",
        ")\n",
        "\n",
        "# Validation MLTable with v1 TabularDataset\n",
        "my_validation_data_input = Input(\n",
        "    type=AssetTypes.MLTABLE, path=\"azureml:odFridgeObjectsValidationDataset:1\",\n",
        "    mode=InputOutputModes.DIRECT\n",
        ")\n",
        "\"\"\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "data-load-v1",
        "gather": {
          "logged": 1712796365978
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. コンピュートターゲットの設定\n",
        "\n",
        "AutoML モデルのトレーニングに使用される[コンピュートターゲット](https://docs.microsoft.com/ja-jp/azure/machine-learning/concept-azure-machine-learning-architecture#computes)を提供する必要があります。画像タスク用の AutoML モデルには、NC、NCv2、NCv3、ND、NDv2、NCasT4 シリーズなどの[GPU SKU](https://docs.microsoft.com/ja-jp/azure/virtual-machines/sizes-gpu)が必要です。より高速なトレーニングのために、NCsv3 シリーズ（v100 GPUを搭載）の使用を推奨します。マルチ GPU VM SKUを備えたコンピュート ターゲットを使用すると、複数の GPU を活用してトレーニング速度を向上させることができます。さらに、複数のノードを備えたコンピュート ターゲットを設定すると、モデルのハイパーパラメータをチューニングする際に並列性を活用して、より速くモデルトレーニングを行うことができます。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import AmlCompute\n",
        "from azure.core.exceptions import ResourceNotFoundError\n",
        "\n",
        "compute_name = \"gpu-cluster-nc6s\"\n",
        "\n",
        "try:\n",
        "    _ = ml_client.compute.get(compute_name)\n",
        "    print(\"Found existing compute target.\")\n",
        "except ResourceNotFoundError:\n",
        "    print(\"Creating a new compute target...\")\n",
        "    compute_config = AmlCompute(\n",
        "        name=compute_name,\n",
        "        type=\"amlcompute\",\n",
        "        size=\"Standard_NC6s_v3\",\n",
        "        idle_time_before_scale_down=120,\n",
        "        min_instances=0,\n",
        "        max_instances=4,\n",
        "    )\n",
        "    ml_client.begin_create_or_update(compute_config).result()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [
          "validation-compute"
        ],
        "gather": {
          "logged": 1712796366383
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. オブジェクト検出トレーニングのための AutoML の設定と実行\n",
        "\n",
        "AutoML を使用すると、画像データに対して画像分類、オブジェクト検出、セグメンテーションのモデルを簡単にトレーニングすることができます。使用するモデルアルゴリズムとハイパーパラメータを制御したり、手動で指定したハイパーパラメータ空間上でのスイープを実行したり、システムによって自動的にハイパーパラメータスイープを行わせることができます。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1. モデルの自動ハイパーパラメータスイープ（AutoMode）\n",
        "\n",
        "AutoML for Imagesを使用する際、最適なモデルを見つけるために自動ハイパーパラメータスイープを実行することができます（この機能をAutoModeと呼びます）。最初のベースラインモデルを取得するには、最初に自AutoModeで実施することをお勧めします。AutoMLにより、モデルアーキテクチャや学習率（learning_rate）、エポック数（number_of_epochs）、トレーニングバッチサイズ（training_batch_size）などの値が決定され、複数パターン自動で試行されます。ハイパーパラメータの検索空間、サンプリング方法、早期終了ポリシーを指定する必要はありません。多くのデータセットに対して、10から20回の実行でうまく機能するでしょう。\n",
        "\n",
        "AutoModeは、`max_trials`を1より大きい値に設定し、ハイパーパラメータ空間、サンプリング方法、終了ポリシーを省略することでトリガーされます。\n",
        "\n",
        "以下の関数は、自動スイープのためのAutoMLジョブを設定します：\n",
        "### image_object_detection() 関数のパラメータ:\n",
        "`image_object_detection()`関数を使用すると、トレーニングジョブを設定できます。\n",
        "\n",
        "- `compute` - AutoMLジョブが実行されるコンピュート。この例では、ワークスペースに存在する'gpu-cluster'というコンピュートを使用しています。ワークスペース内の他のコンピュートに置き換えることができます。\n",
        "- `experiment_name` - 実験の名前。実験は、論理的な機械学習実験に関連する複数の実行を含むAzure MLワークスペース内のフォルダのようなものです。\n",
        "- `name` - ジョブ/実行の名前。これはオプションのプロパティです。指定されていない場合、ランダムな名前が生成されます。\n",
        "- `primary_metric` - AutoMLがモデル選択のために最適化するメトリック。\n",
        "- `target_column_name` - 予測の対象となる列の名前。常に指定する必要があります。このパラメータは'training_data'および'validation_data'に適用されます。\n",
        "- `training_data` - トレーニングに使用されるデータ。トレーニング特徴列とターゲット列の両方を含むべきです。オプションで、このデータはバリデーションまたはテストデータセットを分離するために分割することができます。\n",
        "ワークスペースで登録されたMLTableを'<mltable_name>:<version>'の形式で使用することも、ローカルファイルやフォルダをMLTableとして使用することもできます。例：Input(mltable='my_mltable:1')またはInput(mltable=MLTable(local_path=\"./data\"))\n",
        "`training_data`パラメータは常に提供される必要があります。\n",
        "\n",
        "### set_limits() 関数のパラメータ:\n",
        "これは、タイムアウトなどの制限パラメータを設定するためのオプションの設定方法です。\n",
        "\n",
        "- `timeout_minutes` - AutoMLジョブのタイムアウト時間（分）です。指定されていない場合、ジョブの総タイムアウトのデフォルトは6日間（8,640分）です。\n",
        "- `max_trials` - スイープする最大の設定数。1から1000の間の整数でなければなりません。特定のモデルアルゴリズムのデフォルトハイパーパラメータのみを探索する場合、このパラメータを1に設定します。デフォルト値は1です。\n",
        "- `max_concurrent_trials` - 同時に実行できる最大実行数。指定されていない場合、すべての実行が並行して開始されます。指定される場合、1から100の間の整数でなければなりません。デフォルト値は1です。\n",
        "    注：同時実行数は、指定されたコンピュートターゲットで利用可能なリソースによって制限されます。要件に基づいて、コンピュートターゲットに十分なリソースがあることを確認してください。"
      ],
      "metadata": {
        "tags": [
          "validation-remove"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# general job parameters\n",
        "exp_name = \"dpv2-image-object-detection-experiment\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1712796366749
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the AutoML job with the related factory-function.\n",
        "\n",
        "image_object_detection_job = automl.image_object_detection(\n",
        "    compute=compute_name,\n",
        "    experiment_name=exp_name,\n",
        "    training_data=my_training_data_input,\n",
        "    validation_data=my_validation_data_input,\n",
        "    target_column_name=\"label\",\n",
        "    primary_metric=\"mean_average_precision\",\n",
        "    tags={\"my_custom_tag\": \"My custom value\"},\n",
        ")\n",
        "\n",
        "image_object_detection_job.set_limits(\n",
        "    max_trials=10,\n",
        "    max_concurrent_trials=2,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [
          "validation-remove"
        ],
        "gather": {
          "logged": 1712796367260
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computer Vision 用の AutoML Job を Submit する\n",
        "ジョブの設定が完了したら、トレーニングデータセットを使ってビジョンモデルをトレーニングするために、ジョブを送信します。"
      ],
      "metadata": {
        "tags": [
          "validation-remove"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Submit the AutoML job\n",
        "returned_job = ml_client.jobs.create_or_update(\n",
        "    image_object_detection_job\n",
        ")  # submit the job to the backend\n",
        "\n",
        "print(f\"Created job: {returned_job}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [
          "validation-remove"
        ],
        "gather": {
          "logged": 1712796368406
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client.jobs.stream(returned_job.name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [
          "validation-remove"
        ],
        "gather": {
          "logged": 1712371514720
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2. 【オプション】個別の実行\n",
        "\n",
        "AutoMode が要件に合わない場合、モデルアルゴリズムを探索するために個別の実行を行うことができます。各アルゴリズムに対して適切なデフォルトのハイパーパラメータを提供しています。また、同じモデルアルゴリズムに対して異なるハイパーパラメータの組み合わせで個別の実行を行うこともできます。モデルアルゴリズムは`model_name`パラメータを使用して指定されます。サポートされているモデルアルゴリズムのリストについては、[ドキュメント](https://docs.microsoft.com/ja-jp/azure/machine-learning/how-to-auto-train-image-models?tabs=CLI-v2#configure-model-algorithms-and-hyperparameters)を参照してください。\n",
        "\n",
        "以下の関数を使用して、個別の実行のためのAutoMLジョブを設定できます：\n",
        "### set_training_parameters() 関数のパラメータ:\n",
        "これは、パラメータ空間のスイープ中に変更されない固定設定やパラメータを設定するためのオプションの設定方法です。この関数の主なパラメータには以下が含まれます：\n",
        "\n",
        "- `model_name` - トレーニングジョブで使用したいMLアルゴリズムの名前。サポートされているモデルアルゴリズムについては、この[ドキュメント](https://docs.microsoft.com/ja-jp/azure/machine-learning/how-to-auto-train-image-models?tabs=CLI-v2#supported-model-algorithms)を参照してください。\n",
        "- `number_of_epochs` - トレーニングのエポック数。正の整数でなければなりません（デフォルト値は15）。\n",
        "- `layers_to_freeze` - 転移学習のために凍結するモデルのレイヤー数。正の整数でなければなりません（デフォルト値は0）。\n",
        "- `early_stopping` - トレーニング中に早期終了ロジックを有効にします。ブール値でなければなりません（デフォルトはTrue）。\n",
        "- `optimizer` - トレーニングで使用するオプティマイザーのタイプ。sgd、adam、adamwのいずれかでなければなりません（デフォルトはsgd）。\n",
        "- `distributed` - 計算ターゲットが複数のGPUを含む場合に分散トレーニングを有効にします。ブール値でなければなりません（デフォルトはTrue）。\n",
        "\n",
        "特定のアルゴリズム（例えば`yolov5`）のデフォルトのハイパーパラメータ値を使用したい場合、AutoML Image実行のジョブを次のように指定できます："
      ],
      "metadata": {
        "tags": [
          "validation-remove"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the AutoML job with the related factory-function.\n",
        "\n",
        "image_object_detection_job = automl.image_object_detection(\n",
        "    compute=compute_name,\n",
        "    experiment_name=exp_name,\n",
        "    training_data=my_training_data_input,\n",
        "    validation_data=my_validation_data_input,\n",
        "    target_column_name=\"label\",\n",
        ")\n",
        "\n",
        "# Set limits\n",
        "image_object_detection_job.set_limits(timeout_minutes=60)\n",
        "\n",
        "# Pass the fixed settings or parameters\n",
        "image_object_detection_job.set_training_parameters(model_name=\"yolov5\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [
          "validation-remove"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Submit the AutoML job\n",
        "returned_job = ml_client.jobs.create_or_update(image_object_detection_job)\n",
        "\n",
        "print(f\"Created job: {returned_job}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [
          "validation-remove"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client.jobs.stream(returned_job.name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [
          "validation-remove"
        ]
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2.1 MMDetectionのモデルを使用した個別実行（プレビュー）\n",
        "\n",
        "AutoMLがネイティブにサポートするモデルに加えて、オブジェクト検出をサポートするMMDetectionバージョン2.28.2の任意のモデルを使用して個別実行を行うことができます。使用可能なモデルのリストについては、この[ドキュメント](https://github.com/open-mmlab/mmdetection/blob/v2.28.2/docs/en/model_zoo.md)を参照してください。\n",
        "\n",
        "Azure Machine Learning ではモデルを管理するマネージドレジストリがあり、そこから厳選された MMDetection のモデルを利用できます。これらのモデルには、合理的なデフォルトのハイパーパラメーターを提供しています。厳選されたモデルのリストを以下のコードスニペットを使用して取得することができます。"
      ],
      "metadata": {
        "tags": [
          "validation-remove"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "registry_ml_client = MLClient(credential, registry_name=\"azureml\")\n",
        "\n",
        "models = registry_ml_client.models.list()\n",
        "object_detection_models = []\n",
        "for model in models:\n",
        "    try:\n",
        "        model = registry_ml_client.models.get(model.name, label=\"latest\")\n",
        "        if model.tags.get(\"task\", \"\") == \"object-detection\":\n",
        "            object_detection_models.append(model.name)\n",
        "    except Exception as ex:\n",
        "        print(f\"Error while accessing registry model list: {ex}\")\n",
        "\n",
        "object_detection_models"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [
          "validation-remove"
        ]
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "あるモデル（例えば、`vfnet_r50_fpn_mdconv_c3-c5_mstrain_2x_coco`）を試したい場合、AutoML Imageの実行ジョブを次のように指定します："
      ],
      "metadata": {
        "tags": [
          "validation-remove"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the AutoML job with the related factory-function.\n",
        "\n",
        "image_object_detection_job = automl.image_object_detection(\n",
        "    compute=compute_name,\n",
        "    experiment_name=exp_name,\n",
        "    training_data=my_training_data_input,\n",
        "    validation_data=my_validation_data_input,\n",
        "    target_column_name=\"label\",\n",
        ")\n",
        "\n",
        "# Set limits\n",
        "image_object_detection_job.set_limits(timeout_minutes=60)\n",
        "\n",
        "# Pass the fixed settings or parameters\n",
        "image_object_detection_job.set_training_parameters(\n",
        "    model_name=\"vfnet_r50_fpn_mdconv_c3-c5_mstrain_2x_coco\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [
          "validation-remove"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Submit the AutoML job\n",
        "returned_job = ml_client.jobs.create_or_update(image_object_detection_job)\n",
        "\n",
        "print(f\"Created job: {returned_job}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [
          "validation-remove"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client.jobs.stream(returned_job.name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [
          "validation-remove"
        ]
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3. モデルの手動ハイパーパラメーター探索\n",
        "\n",
        "AutoML for Computer Vision を使用する際には、AutoML により定義されたパラメータ空間を超えて、利用者の設定でハイパーパラメーター探索を行い、最適なモデルを見つけることもできます。この例では、COCOという、20万枚以上のラベル付き画像と80以上のラベルカテゴリーを含む大規模なオブジェクト検出、セグメンテーション、キャプションデータセットに事前学習された`yolov5`および`fasterrcnn_resnet50_fpn`モデルのハイパーパラメーターを探索します。学習率（learning_rate）、最適化アルゴリズム（optimizer）、学習率スケジューラ（lr_scheduler）など、さまざまな値の範囲から選択し、最適な'mean_average_precision'を持つモデルを生成します。ハイパーパラメーターの値が指定されていない場合は、指定されたアルゴリズムのデフォルト値が使用されます。\n",
        "\n",
        "set_sweep関数は、探索設定を構成するために使用されます：\n",
        "### set_sweep() パラメーター:\n",
        "- `sampling_algorithm` - 定義されたパラメータ空間を探索するために使用するサンプリング方法。サポートされているサンプリング方法のリストについては、この[ドキュメント](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-image-models?tabs=SDK-v2#sampling-methods-for-the-sweep)を参照してください。\n",
        "- `early_termination` - 性能が低い実行を早期に終了させるポリシー。終了ポリシーが指定されていない場合、すべての設定は完了するまで実行されます。サポートされている早期終了ポリシーについては、この[ドキュメント](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-image-models?tabs=SDK-v2#early-termination-policies)を参照してください。\n",
        "\n",
        "このパラメータ空間からランダムサンプリングを使用してサンプルを選択し、これらの異なるサンプルで合計10回のイテレーションを試み、計算ターゲットで一度に2回のイテレーションを実行します。パラメータ空間が持つパラメータが多いほど、最適なモデルを見つけるためにはより多くのイテレーションが必要であることに注意してください。\n",
        "\n",
        "Bandit policy を利用し、パフォーマンスが低い設定（最も良いパフォーマンスの設定から20%のスラック内にないもの）を終了させることで、計算リソースを大幅に節約します。\n",
        "\n",
        "モデルとハイパーパラメーター探索の詳細については、[ドキュメント](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)を参照してください。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the AutoML job with the related factory-function.\n",
        "\n",
        "image_object_detection_job = automl.image_object_detection(\n",
        "    compute=compute_name,\n",
        "    experiment_name=exp_name,\n",
        "    training_data=my_training_data_input,\n",
        "    validation_data=my_validation_data_input,\n",
        "    target_column_name=\"label\",\n",
        "    primary_metric=ObjectDetectionPrimaryMetrics.MEAN_AVERAGE_PRECISION,\n",
        "    tags={\"my_custom_tag\": \"My custom value\"},\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1634852262026
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "name": "image-object-detection-configuration",
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "validation-scenario"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set limits\n",
        "image_object_detection_job.set_limits(\n",
        "    timeout_minutes=60,\n",
        "    max_trials=10,\n",
        "    max_concurrent_trials=2,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "limit-settings",
        "tags": [
          "validation-trials"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass the fixed settings or parameters\n",
        "image_object_detection_job.set_training_parameters(\n",
        "    early_stopping=True, evaluation_frequency=1\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "pass-arguments"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure sweep settings\n",
        "image_object_detection_job.set_sweep(\n",
        "    sampling_algorithm=\"random\",\n",
        "    early_termination=BanditPolicy(\n",
        "        evaluation_interval=2, slack_factor=0.2, delay_evaluation=6\n",
        "    ),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "sweep-settings"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define search space\n",
        "image_object_detection_job.extend_search_space(\n",
        "    [\n",
        "        SearchSpace(\n",
        "            model_name=Choice([\"yolov5\"]),\n",
        "            learning_rate=Uniform(0.0001, 0.01),\n",
        "            model_size=Choice([\"small\", \"medium\"]),  # model-specific\n",
        "            # image_size=Choice(640, 704, 768),  # model-specific; might need GPU with large memory\n",
        "        ),\n",
        "        SearchSpace(\n",
        "            model_name=Choice([\"fasterrcnn_resnet50_fpn\"]),\n",
        "            learning_rate=Uniform(0.0001, 0.001),\n",
        "            optimizer=Choice([\"sgd\", \"adam\", \"adamw\"]),\n",
        "            min_size=Choice([600, 800]),  # model-specific\n",
        "            # warmup_cosine_lr_warmup_epochs=Choice([0, 3]),\n",
        "        ),\n",
        "    ]\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "search-space-settings"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Submit the AutoML job\n",
        "returned_job = ml_client.jobs.create_or_update(\n",
        "    image_object_detection_job\n",
        ")  # submit the job to the backend\n",
        "\n",
        "print(f\"Created job: {returned_job}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1634852267930
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "name": "submit-run",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client.jobs.stream(returned_job.name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "ハイパーパラメーター探索を行う際には、HyperDrive UI を使用して試行されたさまざまな設定を視覚化することが役立ちます。この UI にアクセスするには、上記のメイン automl イメージジョブのUI内の「Child jobs」タブに移動します。これが HyperDrive の「Parent Run」です。次に、HyperDrive の「Parent Run」の「Trials」タブに移動します。あるいは、こちらから直接 HyperDrive の「Parent Run」を表示し、「Trials」タブに移動することもできます："
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "hd_job = ml_client.jobs.get(returned_job.name + \"_HD\")\n",
        "hd_job"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3.1 MMDetection のモデルを使用した手動ハイパーパラメーター探索（プレビュー）\n",
        "\n",
        "個別実行で MMDetection バージョン2.28.2の任意のモデルを使用できるのと同様に、これらのモデルを使用してハイパーパラメーター探索を行うこともできます。[AutoML](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-image-models?tabs=CLI-v2#configure-model-algorithms-and-hyperparameters)にネイティブにサポートされているモデルと[MMDetection](https://github.com/open-mmlab/mmdetection/blob/v2.28.2/docs/en/model_zoo.md)からのモデルの組み合わせを選択することもできます。\n",
        "\n",
        "この例では、`deformable_detr_twostage_refine_r50_16x2_50e_coco`、`sparse_rcnn_r50_fpn_300_proposals_crop_mstrain_480-800_3x_coco`、`yolov5`のモデルを対象に、学習率（learning_rate）、モデルサイズ（model_size）など、一連の値から選択して、最適な'MeanAveragePrecision'を持つモデルを生成します。"
      ],
      "metadata": {
        "tags": [
          "validation-remove"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the AutoML job with the related factory-function.\n",
        "\n",
        "image_object_detection_job = automl.image_object_detection(\n",
        "    compute=compute_name,\n",
        "    experiment_name=exp_name,\n",
        "    training_data=my_training_data_input,\n",
        "    validation_data=my_validation_data_input,\n",
        "    target_column_name=\"label\",\n",
        "    primary_metric=ObjectDetectionPrimaryMetrics.MEAN_AVERAGE_PRECISION,\n",
        "    tags={\"my_custom_tag\": \"My custom value\"},\n",
        ")\n",
        "\n",
        "# Set limits\n",
        "image_object_detection_job.set_limits(\n",
        "    timeout_minutes=240,\n",
        "    max_trials=10,\n",
        "    max_concurrent_trials=2,\n",
        ")\n",
        "\n",
        "# Configure sweep settings\n",
        "image_object_detection_job.set_sweep(\n",
        "    sampling_algorithm=\"random\",\n",
        "    early_termination=BanditPolicy(\n",
        "        evaluation_interval=2, slack_factor=0.2, delay_evaluation=6\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Define search space\n",
        "image_object_detection_job.extend_search_space(\n",
        "    [\n",
        "        SearchSpace(\n",
        "            model_name=Choice([\"yolov5\"]),\n",
        "            learning_rate=Uniform(0.0001, 0.01),\n",
        "            model_size=Choice([\"small\", \"medium\"]),  # model-specific\n",
        "        ),\n",
        "        SearchSpace(\n",
        "            model_name=Choice(\n",
        "                [\n",
        "                    \"deformable_detr_twostage_refine_r50_16x2_50e_coco\",\n",
        "                    \"sparse_rcnn_r50_fpn_300_proposals_crop_mstrain_480-800_3x_coco\",\n",
        "                ]\n",
        "            ),\n",
        "            learning_rate=Uniform(0.00001, 0.0001),\n",
        "            number_of_epochs=Choice([15, 20]),\n",
        "        ),\n",
        "    ]\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [
          "validation-remove"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Submit the AutoML job\n",
        "returned_job = ml_client.jobs.create_or_update(\n",
        "    image_object_detection_job\n",
        ")  # submit the job to the backend\n",
        "\n",
        "print(f\"Created job: {returned_job}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [
          "validation-remove"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client.jobs.stream(returned_job.name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [
          "validation-remove"
        ]
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. 最良モデルの取得\n",
        "MLFLowClient を使用して、完了した AutoML トライアルの結果（モデル、成果物、メトリクスなど）にアクセスします。\n",
        "\n",
        "Azure Machine Learning ワークスペースは、MLflow と互換性があります。つまり、追加の構成なしで MLflow サーバーとして機能できます。 各ワークスペースには、MLflow によってワークスペースに接続するために使用される MLflow 追跡 URI があります。 Azure Machine Learning ワークスペースは、MLflow と連携するように既に構成されているため、追加の構成は不要です。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLFlow クライアントの初期化\n",
        "\n",
        "AutoML によって作成されたモデルと成果物には MLFlow のインターフェースからアクセスできます。\n",
        "\n",
        "ここで MLFlow クライアントを初期化し、MLFlow クライアント経由でバックエンドを Azure ML に設定します。\n",
        "\n",
        "\n",
        "重要: MLFlow の最新ライブラリをインストールする必要があります。\n",
        "\n",
        "    pip install azureml-mlflow\n",
        "\n",
        "    pip install mlflow"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!/anaconda/envs/azureml_py310_sdkv2/bin/pip install azureml-mlflow\n",
        "!/anaconda/envs/azureml_py310_sdkv2/bin/pip install mlflow"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLFlow の tracking URIを取得する"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "\n",
        "# Obtain the tracking URL from MLClient\n",
        "MLFLOW_TRACKING_URI = ml_client.workspaces.get(\n",
        "    name=ml_client.workspace_name\n",
        ").mlflow_tracking_uri\n",
        "\n",
        "print(MLFLOW_TRACKING_URI)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "get_mlflow_tracking_uri",
        "gather": {
          "logged": 1712381539151
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the MLFLOW TRACKING URI\n",
        "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
        "print(f\"\\nCurrent tracking uri: {mlflow.get_tracking_uri()}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "set_mlflow_tracking_uri",
        "gather": {
          "logged": 1712381539438
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mlflow.tracking.client import MlflowClient\n",
        "\n",
        "# Initialize MLFlow client\n",
        "mlflow_client = MlflowClient()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1712381542252
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AutoML parent Job を取得する"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "job_name = returned_job.name\n",
        "\n",
        "# Example if providing an specific Job name/ID\n",
        "# job_name = \"salmon_camel_5sdf05xvb3\"\n",
        "\n",
        "# Get the parent run\n",
        "mlflow_parent_run = mlflow_client.get_run(job_name)\n",
        "\n",
        "print(\"Parent Run: \")\n",
        "print(mlflow_parent_run)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "parent_run",
        "gather": {
          "logged": 1712381545578
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print parent run tags. 'automl_best_child_run_id' tag should be there.\n",
        "print(mlflow_parent_run.data.tags)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1712381545901
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AutoML の最良のジョブ（試行）を取得する"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the best model's child run\n",
        "\n",
        "best_child_run_id = mlflow_parent_run.data.tags[\"automl_best_child_run_id\"]\n",
        "print(f\"Found best child run id: {best_child_run_id}\")\n",
        "\n",
        "best_run = mlflow_client.get_run(best_child_run_id)\n",
        "\n",
        "print(\"Best child run: \")\n",
        "print(best_run)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "best_run",
        "gather": {
          "logged": 1712381548982
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 最良モデルのメトリクスを取得する\n",
        "実行したAutoMLのメトリクスにアクセスします。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(best_run.data.metrics, index=[0]).T"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1712381555339
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 最良モデルをダウンロード\n",
        "実行したAutoMLの結果（モデル、成果物、メトリクスなど）にアクセスします。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Create local folder\n",
        "local_dir = \"./artifact_downloads\"\n",
        "if not os.path.exists(local_dir):\n",
        "    os.mkdir(local_dir)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "create_local_dir",
        "gather": {
          "logged": 1712381563421
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download run's artifacts/outputs\n",
        "local_path = mlflow_client.download_artifacts(\n",
        "    best_run.info.run_id, \"outputs\", local_dir\n",
        ")\n",
        "print(f\"Artifacts downloaded in: {local_path}\")\n",
        "print(f\"Artifacts: {os.listdir(local_path)}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "download_model",
        "gather": {
          "logged": 1712381573154
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "mlflow_model_dir = os.path.join(local_dir, \"outputs\", \"mlflow-model\")\n",
        "\n",
        "# Show the contents of the MLFlow model folder\n",
        "os.listdir(mlflow_model_dir)\n",
        "\n",
        "# You should see a list of files such as the following:\n",
        "# ['artifacts', 'conda.yaml', 'MLmodel', 'python_env.yaml', 'python_model.pkl', 'requirements.txt']"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1712381629751
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. 最良モデルを登録・デプロイする"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1 マネージドオンラインエンドポイントを作成\n",
        "マネージドオンラインエンドポイントを作成します。マネージドオンラインエンドポイントはリアルタイムの推論リクエストの処理をサーバレスで提供します。\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "from azure.ai.ml.entities import (\n",
        "    ManagedOnlineEndpoint,\n",
        "    ManagedOnlineDeployment,\n",
        "    Model,\n",
        "    Environment,\n",
        "    CodeConfiguration,\n",
        "    ProbeSettings,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "import_endpoint_lib",
        "gather": {
          "logged": 1712381755699
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a unique endpoint name with current datetime to avoid conflicts\n",
        "import datetime\n",
        "\n",
        "online_endpoint_name = \"od-fridge-items-\" + datetime.datetime.now().strftime(\n",
        "    \"%m%d%H%M%f\"\n",
        ")\n",
        "\n",
        "# create an online endpoint\n",
        "endpoint = ManagedOnlineEndpoint(\n",
        "    name=online_endpoint_name,\n",
        "    description=\"this is a sample online endpoint for deploying model\",\n",
        "    auth_mode=\"key\",\n",
        "    tags={\"foo\": \"bar\"},\n",
        ")\n",
        "print(online_endpoint_name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "endpoint",
        "gather": {
          "logged": 1712381760861
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client.begin_create_or_update(endpoint).result()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "create_endpoint",
        "gather": {
          "logged": 1712381859261
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2 最良モデルを登録・デプロイ"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Register model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"od-fridge-items-mlflow-model\"\n",
        "model = Model(\n",
        "    path=f\"azureml://jobs/{best_run.info.run_id}/outputs/artifacts/outputs/mlflow-model/\",\n",
        "    name=model_name,\n",
        "    description=\"my sample object detection model\",\n",
        "    type=AssetTypes.MLFLOW_MODEL,\n",
        ")\n",
        "\n",
        "# for downloaded file\n",
        "# model = Model(\n",
        "#     path=mlflow_model_dir,\n",
        "#     name=model_name,\n",
        "#     description=\"my sample object detection model\",\n",
        "#     type=AssetTypes.MLFLOW_MODEL,\n",
        "# )\n",
        "\n",
        "registered_model = ml_client.models.create_or_update(model)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "register_model",
        "gather": {
          "logged": 1712381869613
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "registered_model.id"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1712381869930
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### デプロイ"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import OnlineRequestSettings\n",
        "\n",
        "# Setting the request timeout to 90 seconds. Please note that if you use a GPU compute, inference would be faster\n",
        "# and this setting may not be required.\n",
        "req_timeout = OnlineRequestSettings(request_timeout_ms=90000)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1712381872447
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deployment = ManagedOnlineDeployment(\n",
        "    name=\"od-fridge-items-mlflow-deploy\",\n",
        "    endpoint_name=online_endpoint_name,\n",
        "    model=registered_model.id,\n",
        "    instance_type=\"Standard_DS4_V2\",\n",
        "    instance_count=1,\n",
        "    request_settings=req_timeout,\n",
        "    liveness_probe=ProbeSettings(\n",
        "        failure_threshold=30,\n",
        "        success_threshold=1,\n",
        "        timeout=2,\n",
        "        period=10,\n",
        "        initial_delay=2000,\n",
        "    ),\n",
        "    readiness_probe=ProbeSettings(\n",
        "        failure_threshold=10,\n",
        "        success_threshold=1,\n",
        "        timeout=10,\n",
        "        period=10,\n",
        "        initial_delay=2000,\n",
        "    ),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "deploy",
        "tags": [
          "validation-deployment"
        ],
        "gather": {
          "logged": 1712381873516
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client.online_deployments.begin_create_or_update(deployment).result()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "create_deploy",
        "gather": {
          "logged": 1712379523116
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# od fridge items deployment to take 100% traffic\n",
        "endpoint.traffic = {\"od-fridge-items-mlflow-deploy\": 100}\n",
        "ml_client.begin_create_or_update(endpoint).result()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "update_traffic",
        "gather": {
          "logged": 1712384679298
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### エンドポイントの詳細情報を取得"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the details for online endpoint\n",
        "endpoint = ml_client.online_endpoints.get(name=online_endpoint_name)\n",
        "\n",
        "# existing traffic details\n",
        "print(endpoint.traffic)\n",
        "\n",
        "# Get the scoring URI\n",
        "print(endpoint.scoring_uri)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1712384806935
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the deployment"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Create request json\n",
        "import base64\n",
        "\n",
        "sample_image = os.path.join(dataset_dir, \"images\", \"1.jpg\")\n",
        "\n",
        "\n",
        "def read_image(image_path):\n",
        "    with open(image_path, \"rb\") as f:\n",
        "        return f.read()\n",
        "\n",
        "\n",
        "request_json = {\n",
        "    \"input_data\": {\n",
        "        \"columns\": [\"image\"],\n",
        "        \"data\": [base64.encodebytes(read_image(sample_image)).decode(\"utf-8\")],\n",
        "    }\n",
        "}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "create_inference_request",
        "gather": {
          "logged": 1712385116540
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "request_file_name = \"sample_request_data.json\"\n",
        "\n",
        "with open(request_file_name, \"w\") as request_file:\n",
        "    json.dump(request_json, request_file)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "dump_inference_request",
        "gather": {
          "logged": 1712385122155
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resp = ml_client.online_endpoints.invoke(\n",
        "    endpoint_name=online_endpoint_name,\n",
        "    deployment_name=deployment.name,\n",
        "    request_file=request_file_name,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "invoke_inference",
        "gather": {
          "logged": 1712385129912
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize detections\n",
        "Now that we have scored a test image, we can visualize the bounding boxes for this image."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "IMAGE_SIZE = (18, 12)\n",
        "plt.figure(figsize=IMAGE_SIZE)\n",
        "img_np = mpimg.imread(sample_image)\n",
        "img = Image.fromarray(img_np.astype(\"uint8\"), \"RGB\")\n",
        "x, y = img.size\n",
        "\n",
        "fig, ax = plt.subplots(1, figsize=(15, 15))\n",
        "# Display the image\n",
        "ax.imshow(img_np)\n",
        "\n",
        "# draw box and label for each detection\n",
        "detections = json.loads(resp)\n",
        "for detect in detections[0][\"boxes\"]:\n",
        "    label = detect[\"label\"]\n",
        "    box = detect[\"box\"]\n",
        "    conf_score = detect[\"score\"]\n",
        "    if conf_score > 0.6:\n",
        "        ymin, xmin, ymax, xmax = (\n",
        "            box[\"topY\"],\n",
        "            box[\"topX\"],\n",
        "            box[\"bottomY\"],\n",
        "            box[\"bottomX\"],\n",
        "        )\n",
        "        topleft_x, topleft_y = x * xmin, y * ymin\n",
        "        width, height = x * (xmax - xmin), y * (ymax - ymin)\n",
        "        print(\n",
        "            f\"{detect['label']}: [{round(topleft_x, 3)}, {round(topleft_y, 3)}, \"\n",
        "            f\"{round(width, 3)}, {round(height, 3)}], {round(conf_score, 3)}\"\n",
        "        )\n",
        "\n",
        "        color = np.random.rand(3)  #'red'\n",
        "        rect = patches.Rectangle(\n",
        "            (topleft_x, topleft_y),\n",
        "            width,\n",
        "            height,\n",
        "            linewidth=3,\n",
        "            edgecolor=color,\n",
        "            facecolor=\"none\",\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "        plt.text(topleft_x, topleft_y - 10, label, color=color, fontsize=20)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "visualize_detections",
        "gather": {
          "logged": 1712385140197
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### デプロイとエンドポイントを削除"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client.online_endpoints.begin_delete(name=online_endpoint_name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "name": "delte_endpoint"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 次のステップ: 最適なモデルをロードして予測を試す\n",
        "\n",
        "モデルをローカルにロードすることは、ノートブックをモデルと互換性のある環境で実行していることを前提としています。モデルが期待する依存関係のリストは、AutoMLによって生成されたMLFlowモデル（mlflow-modelフォルダ内の'conda.yaml'ファイル）に指定されています。\n",
        "\n",
        "AutoMLモデルが異なる環境でリモートでトレーニングされ、現在このノートブックを実行しているローカルのconda環境とは異なる依存関係を持っているため、モデルをロードしたい場合はいくつかのオプションがあります：\n",
        "\n",
        "1. モデルをメモリ内にローカルでロードして予測を試す推奨される方法は、MLFlowモデルのフォルダ内のconda.yamlファイルに指定されている依存関係で新しい/クリーンなconda環境を作成し、その後MLFlowを使用してモデルをロードし、この同じフォルダ内のノートブック**mlflow-model-local-inference-test.ipynb**に説明されているように.predict()を呼び出すことです。\n",
        "\n",
        "2. Azure ML SDKおよびAutoMLの使用に使用した現在のconda環境に、conda.yamlに指定されているすべてのパッケージ/依存関係をインストールすることもできます。MLflow SDKには、現在の環境に依存関係をインストールする方法もあります。しかし、このオプションは現在の環境にインストールされている内容によっては、パッケージバージョンの競合のリスクがあります。\n",
        "\n",
        "3. また、次のコマンドを使用することもできます：mlflow models serve -m 'xxxxxxx'"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next Steps\n",
        "AutoML で、Regression, Image-Classification, NLP-Text-Classification, Time-Series-Forcasting のようなそのほかのタスクを試してみることも可能です。"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "categories": [
      "SDK v2",
      "sdk",
      "python",
      "jobs",
      "automl-standalone-jobs",
      "automl-image-object-detection-task-fridge-items"
    ]
  },
  "nbformat": 4,
  "nbformat_minor": 4
}